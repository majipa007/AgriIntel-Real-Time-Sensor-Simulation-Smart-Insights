# ğŸŒ¾ AgriIntel â€“ A Weather-Aware Farming Assistant

**AgriIntel** is a personal, learning-driven project that combines real-time data streaming, batch processing, analytics, and GenAI to build a smart assistant for farmers.

The goal is to simulate and process environmental sensor data (like temperature, humidity, and rainfall) to deliver **location-specific farming insights**. Think of it as a platform that helps answer questions like:

* *â€œDo I need to water my crops today?â€*
* *â€œIs this month drier than average?â€*
* *â€œHow should I manage fungal risk for my wheat crop in Lucknow?â€*

---

## ğŸ’¡ Key Features

* **Kafka-based Simulation**
  Simulate weather sensor data using a Kafka producer to mimic real-time data ingestion.

* **Kafka Streaming Pipeline**
  A Kafka consumer captures these events and stores them as structured files (JSON/Parquet).

* **Raw Data Lake**
  Data is stored in a timestamp-organized directory structure for easy batch access and replay.

* **Apache Spark for Batch Processing**
  Periodically transforms and aggregates the raw data (e.g., daily averages, rainfall totals).

* **PostgreSQL as a Serving Layer**
  Cleaned, aggregated results are stored in a relational database for querying.

* **Streamlit Dashboard & LLM Assistant**
  Visualize trends, compare regions, and chat with a FastAPI-based LLM assistant trained on farming rules and weather data.

---

## ğŸ“‚ Expected Folder Structure

```
agriintel/
â”œâ”€â”€ simulator/                  # Sensor simulator (Kafka producer)
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ consumer/                   # Kafka consumer â†’ raw file writer
â”‚   â”œâ”€â”€ consumer.py
â”‚   â””â”€â”€ storage_writer.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw Kafka output (JSON/Parquet)
â”‚   â””â”€â”€ processed/              # Spark-aggregated results
â”‚
â”œâ”€â”€ spark_jobs/                 # Spark batch transformation logic
â”‚   â”œâ”€â”€ aggregate_weather.py
â”‚   â””â”€â”€ job_config.yaml
â”‚
â”œâ”€â”€ database/                   # PostgreSQL schema and diagrams
â”‚   â”œâ”€â”€ init.sql
â”‚   â””â”€â”€ schema_diagram.png
â”‚
â”œâ”€â”€ analytics/                  # Dashboards and notebooks
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ notebook.ipynb
â”‚
â”œâ”€â”€ llm_assistant/              # FastAPI + OpenAI assistant
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ prompts/
â”‚
â””â”€â”€ utils/                      # Shared helper functions
    â”œâ”€â”€ logger.py
    â””â”€â”€ config_loader.py
```

---

## ğŸ“Š High-Level Data Flow

```
[Data Simulator]
      â†“
[Kafka Producer â†’ Kafka Topic]
      â†“
[Kafka Consumer â†’ File Storage]
      â†“
[Apache Spark Batch Jobs]
      â†“
[PostgreSQL Database]
      â†“
[Streamlit Dashboard & LLM Assistant]
```

---

## ğŸ› ï¸ Tools Involved

| Component     | Tech Used               |
| ------------- | ----------------------- |
| Simulation    | Kafka Producer (Python) |
| Ingestion     | Kafka Consumer          |
| Storage (Raw) | Local File System       |
| Processing    | Apache Spark (Batch)    |
| Final Storage | PostgreSQL              |
| Analytics     | Streamlit               |
| LLM Assistant | FastAPI + OpenAI API    |

---